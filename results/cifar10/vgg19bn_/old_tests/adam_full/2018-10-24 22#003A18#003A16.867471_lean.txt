parameters
adam_beta1:0.9
adam_beta2:0.999
adam_eps:1e-08
adam_weight_decay:0.0
adam_amsgrad:False
architecture:vgg19bn_
dataset:cifar10
combine_datasets:False
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:1.0
initial_learning_rate:0.001
executions:1
base_seed:1230
training_method:adam
learning_method:constant

train_loss
1.825,1.477,1.126,0.919,0.774,0.674,0.598,0.542,0.498,0.459,0.421,0.396,0.37,0.343,0.324,0.308,0.286,0.266,0.252,0.242,0.223,0.213,0.203,0.192,0.178,0.173,0.161,0.152,0.144,0.136,0.13,0.127,0.123,0.114,0.106,0.106,0.1,0.096,0.091,0.091,0.087,0.082,0.076,0.079,0.075,0.071,0.066,0.065,0.063,0.064,0.065,0.062,0.058,0.055,0.056,0.054,0.053,0.051,0.049,0.051,0.049,0.048,0.044,0.047,0.042,0.046,0.039,0.042,0.044,0.044,0.041,0.035,0.038,0.039,0.036,0.036,0.034,0.034,0.031,0.035,0.036,0.036,0.031,0.032,0.031,0.029,0.029,0.03,0.029,0.029,0.031,0.025,0.028,0.028,0.027,0.031,0.026,0.023,0.025,0.026

train_acc1
26.666,42.576,59.046,67.574,72.968,76.63,79.604,81.526,83.284,84.532,86.068,86.796,87.638,88.54,89.224,89.638,90.512,91.116,91.52,91.78,92.606,92.836,93.188,93.576,94.072,94.152,94.614,94.944,95.146,95.524,95.616,95.846,95.968,96.284,96.47,96.476,96.804,96.776,96.906,97.026,97.15,97.27,97.494,97.442,97.558,97.648,97.786,97.836,97.896,97.88,97.9,97.976,98.11,98.19,98.168,98.23,98.274,98.288,98.432,98.256,98.394,98.428,98.6,98.44,98.592,98.472,98.708,98.632,98.524,98.626,98.692,98.85,98.806,98.638,98.818,98.844,98.886,98.896,99.0,98.872,98.864,98.794,98.974,99.008,99.028,99.066,99.0,99.0,99.024,99.068,99.002,99.218,99.04,99.084,99.128,99.028,99.136,99.256,99.206,99.178

val_acc1
35.03,43.17,53.77,69.53,69.41,73.6,79.4,77.83,81.96,80.5,83.27,85.29,85.05,85.43,86.31,86.13,85.41,87.33,87.15,86.65,87.98,86.18,88.05,87.99,88.35,89.31,88.88,89.1,88.54,89.23,88.92,89.65,89.16,89.5,89.19,89.75,89.29,89.26,90.04,90.08,89.32,89.52,90.22,90.08,89.74,89.68,89.81,90.08,89.72,90.15,90.51,90.22,89.87,90.02,89.96,90.24,89.91,89.43,89.87,89.74,90.31,90.64,90.05,90.59,90.27,90.43,90.47,90.28,90.06,90.24,90.72,90.03,90.8,90.35,90.7,90.58,90.4,90.45,90.92,90.18,90.53,90.22,90.58,90.86,90.06,91.04,90.39,90.76,90.86,90.4,91.07,90.17,90.1,90.25,90.07,90.42,90.93,90.72,90.33,90.42

val_loss
1.627,1.608,1.36,0.849,0.919,0.786,0.594,0.661,0.556,0.617,0.502,0.451,0.477,0.456,0.42,0.437,0.476,0.406,0.415,0.423,0.395,0.495,0.387,0.395,0.393,0.373,0.378,0.386,0.441,0.404,0.392,0.395,0.408,0.401,0.416,0.406,0.44,0.415,0.385,0.384,0.399,0.463,0.441,0.416,0.427,0.456,0.414,0.412,0.442,0.449,0.405,0.418,0.451,0.441,0.475,0.429,0.458,0.472,0.461,0.476,0.442,0.425,0.458,0.437,0.433,0.423,0.477,0.464,0.446,0.447,0.444,0.487,0.427,0.469,0.434,0.445,0.465,0.485,0.463,0.489,0.452,0.448,0.472,0.427,0.487,0.488,0.479,0.456,0.449,0.469,0.436,0.538,0.508,0.504,0.474,0.438,0.453,0.471,0.504,0.484

learning_rate
0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001

