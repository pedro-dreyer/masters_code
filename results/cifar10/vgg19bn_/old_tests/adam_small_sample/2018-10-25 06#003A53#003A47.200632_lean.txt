parameters
adam_beta1:0.9
adam_beta2:0.999
adam_eps:1e-08
adam_weight_decay:0.0
adam_amsgrad:False
architecture:vgg19bn_
dataset:cifar10
combine_datasets:False
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:0.1
initial_learning_rate:0.001
executions:1
base_seed:1230
training_method:adam
learning_method:constant

train_loss
2.071,1.938,1.92,1.87,1.809,1.806,1.735,1.689,1.651,1.606,1.572,1.605,1.513,1.42,1.391,1.378,1.342,1.335,1.272,1.238,1.219,1.213,1.135,1.1,1.112,1.08,0.976,0.994,1.02,0.911,0.878,0.966,0.973,0.834,0.779,0.77,0.821,0.816,0.78,0.774,0.867,0.833,0.65,0.711,0.677,0.719,0.573,0.618,0.564,0.547,0.654,0.688,0.651,0.524,0.48,0.46,0.474,0.501,0.589,0.771,0.552,0.52,0.398,0.403,0.364,0.371,0.33,0.368,0.369,0.352,0.346,0.407,0.386,0.444,0.396,0.262,0.227,0.223,0.443,0.289,0.245,0.367,0.35,0.27,0.381,0.391,0.372,0.305,0.277,0.24,0.226,0.281,0.251,0.183,0.208,0.187,0.243,0.321,0.175,0.195

train_acc1
17.78,20.2,21.8,24.46,28.88,29.78,31.22,32.58,33.68,34.78,38.34,36.72,38.2,43.18,45.2,45.8,47.96,48.96,51.22,53.9,54.4,53.84,57.76,59.86,59.32,59.16,64.24,63.4,63.66,66.42,68.14,66.38,65.44,69.6,72.0,72.22,72.62,71.58,72.48,72.72,69.62,70.06,77.7,75.08,76.4,75.16,79.7,77.58,80.44,80.02,79.34,76.96,78.12,81.98,84.26,84.02,84.1,83.12,82.68,75.0,83.28,82.14,86.24,85.94,87.64,87.36,88.64,88.4,87.68,88.68,89.1,87.04,87.42,86.34,86.62,90.7,91.84,92.34,86.22,90.28,92.3,88.24,88.92,92.44,88.86,89.58,88.84,91.14,91.56,92.54,92.86,91.82,91.94,94.14,93.52,94.06,93.54,89.4,95.26,95.1

val_acc1
17.01,20.24,21.0,23.0,24.61,28.52,30.63,30.07,26.89,34.55,33.59,36.34,39.9,43.24,31.04,41.3,39.65,43.96,49.81,46.45,48.72,52.25,50.27,51.99,51.04,56.95,56.74,56.74,57.09,60.31,57.04,49.54,58.44,61.58,64.5,63.29,62.6,62.69,65.26,64.43,58.43,64.46,64.35,65.55,63.82,65.7,68.58,66.24,67.46,66.53,63.15,67.36,67.25,69.91,71.4,68.43,70.11,70.33,65.97,68.57,68.19,71.63,68.83,70.82,70.33,71.17,72.24,70.71,72.04,72.97,68.59,72.78,70.08,70.62,69.54,72.64,70.97,72.21,71.72,72.22,69.9,72.49,73.05,72.61,73.44,71.97,71.11,73.53,74.07,73.46,73.63,72.84,72.97,73.63,73.98,74.03,70.62,71.84,73.89,73.74

val_loss
3.665,2.266,2.844,1.938,2.056,1.803,1.716,1.861,2.015,1.648,1.866,1.748,1.516,1.439,1.903,1.642,1.659,1.578,1.354,1.568,1.502,1.307,1.447,1.336,1.365,1.213,1.248,1.288,1.262,1.166,1.226,1.917,1.226,1.141,1.105,1.164,1.15,1.157,1.074,1.088,1.346,1.105,1.18,1.031,1.183,1.076,0.988,1.08,1.107,1.18,1.326,1.045,1.097,0.984,0.993,1.12,1.009,1.022,1.232,1.007,1.145,0.925,1.138,1.037,1.123,1.068,1.037,1.16,1.034,1.049,1.385,1.001,1.059,1.193,1.214,1.052,1.229,1.159,1.113,1.113,1.286,1.131,1.093,1.139,1.012,1.095,1.133,1.039,1.071,1.053,1.121,1.141,1.117,1.136,1.131,1.136,1.391,1.172,1.142,1.179

learning_rate
0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001

