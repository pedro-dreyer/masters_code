parameters
adagrad_learning_decay:0.99
adagrad_weight_decay:0.0
adagrad_initial_acumulator:0.0
architecture:vgg19bn_
dataset:cifar10
combine_datasets:False
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:0.8
initial_learning_rate:0.001
executions:1
base_seed:1230
training_method:adagrad
learning_method:constant

train_loss
2.097,2.068,2.063,2.06,2.058,2.058,2.056,2.056,2.056,2.055,2.056,2.055,2.055,2.054,2.054,2.053,2.054,2.054,2.053,2.053,2.053,2.052,2.052,2.053,2.052,2.053,2.052,2.052,2.052,2.052,2.051,2.052,2.052,2.052,2.052,2.052,2.051,2.051,2.052,2.051,2.051,2.052,2.052,2.052,2.051,2.052,2.051,2.052,2.052,2.051,2.051,2.05,2.051,2.051,2.051,2.051,2.051,2.05,2.051,2.051,2.051,2.051,2.05,2.051,2.051,2.051,2.052,2.051,2.051,2.05,2.05,2.051,2.051,2.051,2.051,2.05,2.051,2.051,2.051,2.051,2.051,2.05,2.05,2.05,2.051,2.051,2.05,2.05,2.05,2.051,2.051,2.05,2.05,2.051,2.051,2.051,2.05,2.051,2.05,2.051

train_acc1
19.422,20.465,20.912,20.995,20.905,21.255,21.002,21.18,21.085,21.157,21.14,21.127,21.195,21.213,21.383,21.395,21.205,21.303,21.378,21.35,21.378,21.415,21.283,21.312,21.467,21.318,21.15,21.367,21.352,21.355,21.352,21.247,21.298,21.422,21.442,21.457,21.33,21.332,21.242,21.255,21.367,21.285,21.273,21.355,21.4,21.41,21.593,21.347,21.345,21.347,21.332,21.398,21.318,21.398,21.322,21.495,21.53,21.26,21.455,21.285,21.305,21.642,21.71,21.415,21.508,21.515,21.273,21.473,21.52,21.488,21.585,21.542,21.44,21.275,21.51,21.59,21.235,21.347,21.44,21.435,21.41,21.312,21.435,21.537,21.522,21.183,21.455,21.455,21.288,21.362,21.412,21.395,21.45,21.442,21.315,21.43,21.32,21.383,21.55,21.4

val_acc1
21.37,21.64,21.92,21.86,22.08,22.26,21.87,21.9,22.14,21.98,22.12,22.11,22.03,22.17,22.22,22.2,21.99,22.32,22.11,22.18,22.34,22.21,22.29,22.42,22.37,22.01,22.09,22.28,22.26,22.34,22.41,22.4,22.3,22.33,22.17,22.46,22.29,22.23,22.36,22.4,22.49,22.44,22.42,22.43,22.33,22.2,22.39,22.44,22.27,22.42,22.37,22.14,22.42,22.41,22.19,22.38,22.36,22.3,22.42,22.24,22.38,22.41,22.07,22.3,22.44,22.53,22.29,22.43,22.45,22.33,22.31,22.16,22.28,22.27,22.45,22.28,22.46,22.55,22.28,22.17,22.35,22.39,22.39,22.52,22.37,22.49,22.48,22.24,22.38,22.45,22.4,22.32,22.4,22.46,22.44,22.47,22.41,22.18,22.44,22.47

val_loss
2.058,2.053,2.047,2.045,2.043,2.041,2.043,2.04,2.039,2.042,2.039,2.041,2.041,2.042,2.04,2.038,2.038,2.041,2.041,2.038,2.037,2.037,2.037,2.04,2.039,2.038,2.038,2.039,2.036,2.036,2.035,2.037,2.038,2.034,2.04,2.039,2.041,2.038,2.037,2.036,2.037,2.036,2.035,2.038,2.037,2.035,2.037,2.036,2.038,2.035,2.038,2.036,2.038,2.037,2.039,2.039,2.037,2.038,2.036,2.036,2.032,2.036,2.038,2.035,2.035,2.037,2.038,2.036,2.036,2.035,2.034,2.033,2.037,2.036,2.036,2.037,2.035,2.036,2.035,2.037,2.034,2.036,2.035,2.035,2.035,2.037,2.037,2.036,2.033,2.034,2.037,2.038,2.035,2.033,2.036,2.034,2.035,2.034,2.037,2.035

learning_rate
0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001

