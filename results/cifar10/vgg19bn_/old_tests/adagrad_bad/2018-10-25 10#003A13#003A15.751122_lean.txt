parameters
adagrad_learning_decay:0.99
adagrad_weight_decay:0.0
adagrad_initial_acumulator:0.0
architecture:vgg19bn_
dataset:cifar10
combine_datasets:False
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:0.8
initial_learning_rate:0.005
executions:1
base_seed:1230
training_method:adagrad
learning_method:constant

train_loss
2.289,2.266,2.261,2.257,2.256,2.255,2.254,2.253,2.252,2.252,2.252,2.252,2.251,2.251,2.251,2.25,2.25,2.251,2.25,2.25,2.25,2.249,2.249,2.249,2.249,2.249,2.249,2.249,2.249,2.249,2.248,2.248,2.249,2.249,2.249,2.248,2.248,2.249,2.248,2.248,2.248,2.249,2.248,2.248,2.248,2.248,2.248,2.248,2.248,2.249,2.248,2.248,2.247,2.247,2.248,2.248,2.248,2.248,2.248,2.247,2.248,2.248,2.247,2.248,2.248,2.247,2.247,2.248,2.247,2.248,2.248,2.247,2.247,2.247,2.248,2.247,2.247,2.247,2.248,2.247,2.247,2.247,2.247,2.246,2.248,2.247,2.247,2.247,2.248,2.247,2.247,2.247,2.247,2.247,2.247,2.247,2.247,2.247,2.247,2.247

train_acc1
10.855,12.152,12.13,12.17,12.255,12.242,12.308,12.313,12.27,12.333,12.32,12.325,12.39,12.285,12.328,12.418,12.335,12.22,12.34,12.325,12.323,12.348,12.305,12.377,12.34,12.295,12.323,12.365,12.435,12.345,12.343,12.385,12.47,12.362,12.33,12.435,12.392,12.438,12.343,12.355,12.428,12.382,12.382,12.455,12.387,12.345,12.403,12.45,12.313,12.362,12.382,12.413,12.415,12.458,12.357,12.418,12.382,12.348,12.377,12.45,12.357,12.37,12.405,12.455,12.335,12.462,12.423,12.365,12.367,12.355,12.365,12.428,12.392,12.485,12.338,12.423,12.413,12.418,12.392,12.34,12.45,12.37,12.423,12.413,12.385,12.403,12.385,12.367,12.405,12.445,12.372,12.392,12.37,12.458,12.413,12.418,12.413,12.518,12.415,12.397

val_acc1
12.82,12.94,12.66,12.69,12.83,12.9,12.95,12.93,12.97,12.88,12.92,12.88,12.98,13.02,12.96,12.94,12.94,13.0,12.99,13.01,13.0,13.03,12.92,12.96,12.97,13.0,13.08,12.99,12.99,13.0,13.01,12.99,13.05,13.02,13.01,12.94,13.03,12.99,13.02,13.02,13.03,12.89,12.94,12.9,12.92,13.02,12.98,13.03,13.03,12.96,13.02,13.02,13.03,12.95,13.03,13.05,13.03,13.03,12.98,13.02,12.98,13.0,13.05,13.05,13.03,13.0,13.04,12.98,12.99,13.04,13.04,13.09,13.05,13.05,12.99,13.09,13.03,13.03,13.05,13.08,12.99,12.95,13.06,13.03,13.04,13.04,12.99,13.02,13.03,13.05,13.08,13.07,12.98,13.05,13.0,13.06,13.02,13.13,12.99,13.06

val_loss
2.266,2.25,2.25,2.249,2.243,2.239,2.236,2.236,2.234,2.238,2.238,2.237,2.235,2.235,2.237,2.234,2.233,2.236,2.235,2.233,2.233,2.232,2.235,2.242,2.237,2.232,2.233,2.236,2.235,2.236,2.234,2.231,2.232,2.231,2.233,2.238,2.236,2.236,2.233,2.234,2.236,2.238,2.236,2.237,2.238,2.232,2.235,2.234,2.232,2.236,2.229,2.231,2.232,2.236,2.232,2.235,2.232,2.232,2.231,2.231,2.235,2.233,2.228,2.23,2.235,2.235,2.231,2.235,2.232,2.234,2.233,2.227,2.232,2.233,2.236,2.23,2.236,2.234,2.232,2.229,2.235,2.235,2.232,2.233,2.232,2.235,2.236,2.232,2.229,2.234,2.232,2.231,2.238,2.228,2.237,2.233,2.234,2.229,2.234,2.232

learning_rate
0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005,0.005

