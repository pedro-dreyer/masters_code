parameters
adagrad_lr:0.001
adagrad_learning_decay:0.99
adagrad_weight_decay:0.0
adagrad_initial_acumulator:0.0
architecture:vgg19bn_
dataset:cifar10
combine_datasets:True
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:0.8
executions:1
base_seed:1230
training_method:adagrad
learning_method:constant

train_loss
2.165,2.143,2.139,2.137,2.136,2.135,2.134,2.133,2.133,2.132,2.132,2.132,2.132,2.131,2.13,2.131,2.131,2.131,2.13,2.13,2.131,2.13,2.13,2.131,2.13,2.13,2.13,2.13,2.129,2.13,2.13,2.129,2.129,2.129,2.129,2.129,2.129,2.129,2.129,2.13,2.129,2.129,2.129,2.129,2.129,2.129,2.129,2.129,2.13,2.129,2.129,2.13,2.128,2.129,2.128,2.13,2.129,2.129,2.13,2.129,2.129,2.128,2.129,2.129,2.128,2.129,2.129,2.128,2.13,2.129,2.129,2.129,2.129,2.129,2.128,2.128,2.129,2.129,2.129,2.129,2.129,2.129,2.129,2.128,2.129,2.129,2.129,2.128,2.129,2.128,2.129,2.128,2.129,2.128,2.129,2.129,2.128,2.129,2.128,2.128

train_acc1
19.57,21.357,21.828,21.685,21.97,21.975,22.078,21.995,21.957,21.908,22.19,21.912,22.005,21.922,22.372,22.067,22.132,21.955,22.208,22.147,21.93,21.957,22.07,22.057,21.988,22.002,22.07,22.18,22.072,22.195,22.037,22.218,22.067,22.142,22.2,22.295,22.147,22.013,22.232,22.07,22.138,22.205,22.14,22.132,21.872,22.12,22.037,22.215,22.118,22.19,21.918,22.042,22.25,22.248,22.132,22.03,22.245,22.17,22.072,22.288,22.04,22.075,22.03,22.215,22.048,21.88,21.95,22.085,22.15,22.03,22.237,21.972,22.108,22.128,22.225,22.203,22.222,21.947,22.1,22.213,22.048,22.002,21.978,22.132,21.925,21.972,22.037,22.18,22.075,22.03,22.383,22.242,21.96,22.105,22.225,22.377,22.332,21.988,22.103,22.18

val_acc1
21.688,21.862,22.312,22.213,22.375,22.0,22.25,22.675,22.55,22.813,22.338,22.925,22.037,22.65,22.862,23.15,22.237,22.488,23.1,22.925,22.988,22.1,22.775,22.7,23.15,22.462,22.338,22.425,22.475,22.662,22.612,22.825,22.588,22.938,22.375,22.412,22.775,22.7,22.813,22.7,22.625,22.138,22.55,22.55,22.975,21.862,22.675,22.888,22.65,22.612,23.288,22.45,23.025,22.625,22.988,22.988,22.675,22.65,22.875,22.562,23.05,22.162,22.838,23.113,22.5,22.525,22.4,22.612,22.375,22.838,22.537,22.962,22.938,22.838,22.112,23.025,22.825,22.4,22.762,22.438,22.725,22.625,22.588,22.85,22.537,23.038,22.412,22.375,22.675,23.162,22.713,22.875,22.562,22.638,22.612,23.038,22.725,22.325,23.075,23.062

val_loss
2.135,2.126,2.125,2.124,2.122,2.122,2.118,2.121,2.121,2.12,2.116,2.119,2.117,2.119,2.117,2.119,2.118,2.117,2.118,2.117,2.116,2.116,2.119,2.118,2.116,2.116,2.118,2.116,2.116,2.119,2.115,2.118,2.116,2.118,2.118,2.115,2.117,2.115,2.116,2.112,2.116,2.115,2.114,2.117,2.116,2.114,2.117,2.117,2.117,2.114,2.118,2.117,2.116,2.118,2.114,2.118,2.115,2.114,2.116,2.116,2.116,2.114,2.112,2.113,2.115,2.116,2.113,2.115,2.116,2.115,2.113,2.117,2.119,2.12,2.114,2.116,2.118,2.112,2.114,2.112,2.118,2.116,2.117,2.118,2.114,2.117,2.113,2.118,2.115,2.114,2.114,2.117,2.116,2.117,2.118,2.117,2.116,2.118,2.116,2.116

learning_rate
0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001

