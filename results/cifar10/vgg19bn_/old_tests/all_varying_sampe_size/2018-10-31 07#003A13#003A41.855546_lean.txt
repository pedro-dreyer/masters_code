parameters
adam_lr:0.001
adam_beta1:0.9
adam_beta2:0.999
adam_eps:1e-08
adam_weight_decay:0.0
adam_amsgrad:False
architecture:vgg19bn_
dataset:cifar10
combine_datasets:True
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:0.8
executions:1
base_seed:1230
training_method:adam
learning_method:constant

train_loss
1.882,1.522,1.22,1.001,0.869,0.761,0.679,0.618,0.567,0.518,0.486,0.449,0.42,0.393,0.362,0.341,0.314,0.298,0.28,0.265,0.249,0.238,0.231,0.207,0.202,0.19,0.184,0.171,0.163,0.152,0.143,0.137,0.132,0.128,0.12,0.113,0.111,0.106,0.1,0.098,0.093,0.095,0.085,0.084,0.079,0.073,0.075,0.068,0.076,0.066,0.065,0.065,0.06,0.057,0.061,0.058,0.061,0.053,0.049,0.054,0.049,0.049,0.047,0.049,0.047,0.048,0.046,0.046,0.043,0.043,0.043,0.043,0.035,0.042,0.039,0.04,0.036,0.038,0.036,0.036,0.038,0.035,0.035,0.036,0.029,0.032,0.027,0.035,0.032,0.032,0.03,0.031,0.03,0.027,0.029,0.027,0.029,0.027,0.026,0.026

train_acc1
24.557,39.685,53.89,63.44,69.218,73.73,76.808,79.228,81.258,83.172,83.998,85.275,86.235,87.252,88.038,88.855,89.732,90.23,90.88,91.455,91.905,92.212,92.565,93.282,93.565,93.808,94.055,94.555,94.73,95.095,95.438,95.667,95.718,95.862,96.105,96.325,96.438,96.612,96.732,96.78,96.908,97.022,97.242,97.298,97.45,97.628,97.557,97.695,97.588,97.838,97.888,97.942,98.025,98.138,98.05,98.128,98.015,98.25,98.448,98.25,98.45,98.462,98.432,98.42,98.47,98.45,98.5,98.528,98.64,98.54,98.575,98.585,98.812,98.682,98.748,98.745,98.852,98.832,98.855,98.815,98.752,98.838,98.85,98.825,99.062,98.96,99.105,98.842,99.008,99.042,99.07,98.99,99.07,99.175,99.075,99.158,99.04,99.128,99.15,99.19

val_acc1
29.625,47.988,57.8,58.238,69.038,70.175,74.438,78.475,78.512,81.525,81.225,82.0,80.725,83.05,84.1,82.725,85.288,84.975,83.6,86.112,85.85,85.55,86.788,85.888,87.262,86.4,87.575,87.138,86.075,87.5,87.225,86.988,86.625,87.712,88.388,87.65,87.525,88.1,87.488,87.75,87.775,88.075,87.975,88.788,88.712,87.838,88.438,87.638,88.125,88.05,87.688,88.525,88.95,87.712,88.862,88.762,88.587,88.775,88.35,88.038,88.075,88.7,87.85,88.075,89.188,88.75,89.362,88.413,87.988,88.825,88.275,89.262,88.65,87.925,89.138,89.012,89.438,88.825,89.388,88.613,89.362,89.413,88.188,88.988,88.925,89.512,88.975,88.975,88.812,89.212,89.112,89.825,88.888,89.362,89.1,89.338,89.375,89.288,89.525,89.413

val_loss
1.873,1.353,1.113,1.241,0.92,0.89,0.784,0.645,0.651,0.546,0.585,0.575,0.617,0.525,0.491,0.561,0.48,0.482,0.561,0.44,0.451,0.469,0.432,0.478,0.421,0.474,0.432,0.44,0.489,0.45,0.471,0.476,0.494,0.444,0.429,0.475,0.454,0.464,0.493,0.46,0.496,0.459,0.511,0.436,0.455,0.485,0.497,0.509,0.493,0.495,0.555,0.474,0.5,0.54,0.473,0.495,0.503,0.498,0.554,0.545,0.57,0.507,0.551,0.554,0.503,0.515,0.516,0.546,0.566,0.504,0.515,0.493,0.549,0.599,0.5,0.526,0.501,0.516,0.518,0.539,0.484,0.525,0.581,0.508,0.521,0.523,0.541,0.514,0.55,0.548,0.507,0.513,0.527,0.503,0.556,0.534,0.536,0.531,0.582,0.503

learning_rate
0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001

