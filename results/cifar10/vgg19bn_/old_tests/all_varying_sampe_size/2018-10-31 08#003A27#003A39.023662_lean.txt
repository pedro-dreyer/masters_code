parameters
adam_lr:0.001
adam_beta1:0.9
adam_beta2:0.999
adam_eps:1e-08
adam_weight_decay:0.0
adam_amsgrad:False
architecture:vgg19bn_
dataset:cifar10
combine_datasets:True
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:0.4
executions:1
base_seed:1230
training_method:adam
learning_method:constant

train_loss
1.919,1.743,1.532,1.382,1.24,1.086,0.96,0.881,0.808,0.75,0.689,0.645,0.596,0.563,0.529,0.499,0.467,0.433,0.412,0.392,0.368,0.346,0.327,0.312,0.302,0.289,0.266,0.26,0.233,0.239,0.217,0.215,0.195,0.189,0.189,0.177,0.162,0.152,0.153,0.155,0.141,0.133,0.134,0.134,0.126,0.124,0.11,0.114,0.121,0.093,0.104,0.109,0.095,0.092,0.094,0.084,0.086,0.087,0.098,0.089,0.069,0.084,0.076,0.068,0.064,0.075,0.07,0.063,0.072,0.068,0.066,0.076,0.064,0.046,0.054,0.057,0.069,0.043,0.056,0.05,0.044,0.055,0.045,0.05,0.048,0.051,0.045,0.046,0.051,0.049,0.051,0.038,0.04,0.041,0.061,0.042,0.029,0.034,0.035,0.038

train_acc1
22.3,30.74,38.91,47.23,53.225,59.76,64.96,68.225,71.325,73.59,76.25,77.975,79.71,80.975,82.34,83.52,84.43,85.6,86.345,87.03,87.875,88.565,89.38,89.82,90.125,90.695,91.25,91.31,92.335,92.195,92.985,93.03,93.745,93.81,93.77,94.2,94.645,94.945,95.185,94.935,95.5,95.725,95.765,95.705,95.97,96.04,96.355,96.3,96.17,97.05,96.795,96.39,97.035,97.015,97.055,97.34,97.345,97.32,96.915,97.195,97.78,97.395,97.635,97.92,97.99,97.79,97.69,98.05,97.815,97.865,97.98,97.685,98.01,98.57,98.35,98.265,97.73,98.635,98.32,98.47,98.625,98.3,98.47,98.455,98.495,98.315,98.615,98.665,98.51,98.575,98.42,98.835,98.77,98.71,98.155,98.72,99.065,99.0,98.955,98.79

val_acc1
25.725,35.575,36.225,44.625,53.75,59.8,65.65,58.475,72.075,73.4,74.7,75.025,70.375,76.3,77.2,76.8,78.55,79.4,79.325,76.225,81.575,78.85,78.225,80.475,82.8,80.0,82.15,82.15,82.225,83.775,82.65,81.7,82.05,83.225,83.0,84.0,81.4,81.975,83.475,83.65,84.775,83.975,83.275,83.975,84.5,84.975,82.95,82.875,84.875,84.025,84.95,84.0,85.35,84.275,84.25,84.925,83.7,84.125,82.925,84.775,84.9,84.05,84.9,84.1,84.825,84.35,84.8,84.875,84.25,83.925,85.575,84.125,85.775,85.6,84.975,85.05,85.1,84.55,84.825,86.3,85.05,85.35,85.325,85.325,84.275,85.375,84.85,85.225,85.35,84.85,84.05,85.725,85.725,85.5,85.85,84.25,85.15,86.0,85.25,84.95

val_loss
2.053,1.731,1.648,1.533,1.272,1.113,0.972,1.24,0.806,0.765,0.722,0.752,0.929,0.723,0.722,0.719,0.694,0.655,0.653,0.776,0.593,0.68,0.7,0.673,0.595,0.673,0.6,0.632,0.664,0.572,0.611,0.698,0.675,0.69,0.635,0.61,0.693,0.725,0.656,0.635,0.623,0.659,0.783,0.672,0.626,0.668,0.745,0.736,0.686,0.67,0.676,0.71,0.603,0.711,0.64,0.671,0.69,0.683,0.78,0.664,0.74,0.704,0.668,0.738,0.711,0.765,0.708,0.704,0.698,0.765,0.661,0.792,0.682,0.699,0.756,0.758,0.7,0.747,0.754,0.692,0.739,0.726,0.713,0.746,0.841,0.702,0.761,0.724,0.697,0.731,0.808,0.757,0.747,0.753,0.751,0.798,0.785,0.769,0.752,0.752

learning_rate
0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001

