parameters
adam_lr:0.001
adam_beta1:0.9
adam_beta2:0.999
adam_eps:1e-08
adam_weight_decay:0.0
adam_amsgrad:False
architecture:vgg19bn_
dataset:cifar10
combine_datasets:True
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:1.0
executions:1
base_seed:1230
training_method:adam
learning_method:constant

train_loss
1.758,1.296,1.008,0.837,0.725,0.633,0.571,0.519,0.482,0.439,0.41,0.383,0.354,0.334,0.309,0.295,0.271,0.259,0.239,0.224,0.212,0.192,0.182,0.178,0.167,0.159,0.153,0.142,0.138,0.128,0.123,0.116,0.11,0.108,0.1,0.096,0.094,0.087,0.087,0.087,0.079,0.075,0.074,0.07,0.074,0.067,0.063,0.064,0.06,0.06,0.057,0.054,0.054,0.051,0.05,0.051,0.049,0.05,0.047,0.047,0.046,0.045,0.045,0.037,0.04,0.036,0.044,0.039,0.04,0.035,0.037,0.036,0.032,0.037,0.038,0.034,0.033,0.032,0.029,0.032,0.03,0.028,0.032,0.031,0.029,0.027,0.029,0.027,0.029,0.027,0.026,0.029,0.024,0.025,0.026,0.025,0.024,0.025,0.023,0.024

train_acc1
30.06,51.762,64.222,70.962,75.43,78.878,81.182,82.926,84.154,85.716,86.544,87.516,88.416,89.046,90.066,90.438,91.244,91.698,92.188,92.824,93.09,93.688,94.092,94.184,94.542,94.832,95.062,95.362,95.5,95.858,95.982,96.26,96.462,96.51,96.726,96.898,96.92,97.24,97.11,97.182,97.342,97.548,97.708,97.726,97.652,97.856,97.94,97.98,98.126,98.114,98.196,98.23,98.262,98.376,98.422,98.36,98.444,98.39,98.504,98.438,98.546,98.486,98.584,98.79,98.746,98.862,98.648,98.768,98.714,98.85,98.79,98.816,98.966,98.812,98.844,98.948,98.932,99.034,99.018,98.998,99.122,99.138,98.992,98.98,99.072,99.148,99.064,99.164,99.102,99.16,99.146,99.114,99.24,99.19,99.18,99.196,99.26,99.154,99.294,99.216

val_acc1
34.51,57.9,65.41,68.62,73.88,79.6,80.17,79.81,80.11,82.98,84.64,84.77,83.92,86.57,86.82,85.7,85.38,87.3,86.8,88.23,87.78,88.81,87.66,87.88,88.17,88.4,89.03,88.37,88.29,88.05,89.08,88.61,87.8,89.31,88.64,89.41,89.24,89.3,89.57,88.84,88.96,89.0,89.89,89.55,89.93,89.09,88.59,89.06,87.24,89.12,89.62,89.71,89.48,89.75,90.08,90.16,89.69,89.27,89.21,89.48,89.85,89.9,90.28,90.33,89.72,89.58,90.1,89.7,89.71,89.76,89.92,89.57,90.68,89.65,90.85,89.92,89.93,90.42,90.56,89.68,90.49,90.55,90.72,90.2,91.09,89.98,91.08,90.31,90.31,91.27,90.02,90.71,90.65,90.73,90.58,90.1,90.33,90.65,89.62,90.63

val_loss
1.671,1.212,0.978,0.978,0.793,0.61,0.602,0.638,0.629,0.523,0.464,0.495,0.525,0.446,0.412,0.453,0.468,0.432,0.443,0.39,0.412,0.384,0.432,0.416,0.41,0.395,0.398,0.439,0.41,0.422,0.403,0.424,0.466,0.387,0.455,0.417,0.424,0.41,0.404,0.439,0.459,0.418,0.434,0.421,0.411,0.47,0.489,0.475,0.567,0.442,0.429,0.442,0.452,0.411,0.436,0.477,0.467,0.457,0.486,0.458,0.42,0.411,0.417,0.425,0.467,0.529,0.426,0.489,0.452,0.494,0.462,0.465,0.465,0.515,0.426,0.463,0.458,0.447,0.438,0.562,0.457,0.487,0.44,0.453,0.424,0.505,0.444,0.486,0.454,0.426,0.5,0.434,0.465,0.472,0.459,0.505,0.495,0.473,0.536,0.483

learning_rate
0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001

