parameters
adagrad_lr:0.001
adagrad_learning_decay:0.99
adagrad_weight_decay:0.0
adagrad_initial_acumulator:0.0
architecture:vgg19bn_
dataset:cifar10
combine_datasets:True
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:1.0
executions:1
base_seed:1230
training_method:adagrad
learning_method:constant

train_loss
2.083,2.057,2.053,2.051,2.05,2.049,2.048,2.048,2.047,2.046,2.047,2.047,2.046,2.045,2.046,2.045,2.045,2.045,2.045,2.046,2.045,2.044,2.044,2.044,2.044,2.043,2.044,2.043,2.044,2.044,2.043,2.044,2.043,2.043,2.044,2.044,2.043,2.044,2.044,2.043,2.043,2.043,2.043,2.043,2.043,2.044,2.042,2.043,2.043,2.043,2.043,2.043,2.043,2.043,2.043,2.043,2.042,2.043,2.043,2.042,2.042,2.043,2.043,2.042,2.042,2.042,2.042,2.043,2.043,2.043,2.042,2.042,2.042,2.043,2.043,2.042,2.043,2.043,2.042,2.042,2.042,2.042,2.042,2.043,2.042,2.042,2.042,2.042,2.042,2.042,2.042,2.043,2.043,2.043,2.042,2.043,2.043,2.042,2.043,2.043

train_acc1
19.582,21.324,21.828,22.532,22.628,22.924,23.02,23.018,23.186,23.294,23.214,23.17,23.304,23.172,23.356,23.394,23.432,23.48,23.358,23.272,23.332,23.314,23.352,23.266,23.294,23.688,23.384,23.386,23.368,23.4,23.484,23.476,23.646,23.404,23.306,23.328,23.314,23.38,23.536,23.386,23.416,23.32,23.518,23.39,23.366,23.38,23.47,23.384,23.514,23.364,23.362,23.27,23.712,23.412,23.464,23.516,23.424,23.318,23.452,23.518,23.434,23.292,23.312,23.38,23.324,23.338,23.404,23.47,23.356,23.464,23.53,23.338,23.282,23.452,23.274,23.274,23.24,23.254,23.49,23.276,23.392,23.402,23.472,23.298,23.294,23.418,23.394,23.438,23.478,23.362,23.39,23.468,23.41,23.49,23.494,23.362,23.252,23.236,23.23,23.392

val_acc1
20.88,22.24,22.51,23.37,23.24,23.18,23.59,23.37,23.57,23.83,23.8,23.46,23.58,23.61,24.02,23.78,23.92,23.4,23.97,24.03,23.99,23.42,23.82,23.98,23.91,23.52,23.92,23.54,23.63,23.48,24.19,23.92,23.97,23.8,23.51,23.67,23.57,23.88,23.83,24.0,23.77,24.03,23.66,23.48,23.69,23.76,24.02,23.66,23.95,23.67,23.87,23.66,24.04,23.51,23.77,23.85,23.89,24.13,23.79,23.91,23.73,23.48,23.76,23.99,23.96,23.81,23.78,23.75,23.82,24.23,23.77,23.86,23.71,24.02,24.05,23.63,23.94,23.76,23.98,23.72,23.93,24.0,24.21,24.05,23.91,23.99,23.84,23.78,24.17,23.88,24.15,23.91,24.07,23.59,23.75,23.85,23.73,23.77,24.09,24.1

val_loss
2.056,2.049,2.043,2.041,2.04,2.039,2.038,2.039,2.037,2.037,2.04,2.036,2.036,2.039,2.035,2.038,2.036,2.037,2.038,2.039,2.035,2.036,2.036,2.035,2.038,2.035,2.035,2.037,2.035,2.031,2.036,2.036,2.035,2.035,2.037,2.037,2.032,2.034,2.034,2.033,2.035,2.039,2.038,2.034,2.036,2.037,2.036,2.033,2.036,2.035,2.03,2.038,2.031,2.035,2.033,2.032,2.037,2.036,2.037,2.036,2.032,2.033,2.035,2.035,2.036,2.037,2.035,2.036,2.036,2.033,2.035,2.035,2.035,2.037,2.038,2.034,2.033,2.034,2.036,2.039,2.034,2.034,2.033,2.033,2.036,2.031,2.034,2.036,2.033,2.036,2.033,2.033,2.03,2.037,2.034,2.034,2.035,2.035,2.038,2.032

learning_rate
0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001

