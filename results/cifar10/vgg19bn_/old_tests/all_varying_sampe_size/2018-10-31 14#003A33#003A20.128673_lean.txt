parameters
adagrad_lr:0.001
adagrad_learning_decay:0.99
adagrad_weight_decay:0.0
adagrad_initial_acumulator:0.0
architecture:vgg19bn_
dataset:cifar10
combine_datasets:True
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:0.1
executions:1
base_seed:1230
training_method:adagrad
learning_method:constant

train_loss
2.13,2.058,2.039,2.037,2.035,2.035,2.025,2.029,2.019,2.025,2.023,2.017,2.018,2.022,2.013,2.025,2.019,2.02,2.009,2.025,2.015,2.019,2.014,2.015,2.017,2.014,2.015,2.015,2.016,2.012,2.015,2.024,2.022,2.012,2.017,2.01,2.009,2.013,2.013,2.015,2.017,2.015,2.014,2.012,2.013,2.019,2.014,2.006,2.019,2.009,2.006,2.008,2.009,2.017,2.013,2.016,2.013,2.011,2.009,2.011,2.012,2.011,2.008,2.016,2.016,2.007,2.013,2.012,2.009,2.015,2.016,2.013,2.012,2.005,2.014,2.014,2.008,2.018,2.017,2.006,2.01,2.007,2.02,2.011,2.012,2.021,2.007,2.015,2.014,2.017,2.022,2.007,2.01,2.005,2.009,2.005,2.011,2.008,2.008,2.007

train_acc1
18.96,21.4,22.2,22.16,22.6,22.42,22.54,22.94,22.6,22.8,22.38,22.48,23.12,22.9,22.86,22.84,22.8,22.58,22.8,22.66,23.12,22.9,22.92,22.9,22.86,22.98,22.94,23.38,22.84,23.16,22.8,22.68,22.94,22.92,22.46,23.18,22.64,22.76,23.0,23.0,23.08,23.04,23.18,22.94,22.88,23.1,23.18,22.88,22.72,23.08,22.96,23.12,23.16,23.08,23.12,23.1,23.04,22.98,22.96,23.32,22.84,23.06,22.8,22.86,22.92,23.0,22.78,23.0,22.68,22.82,23.02,22.94,22.46,22.82,23.44,22.86,22.96,23.16,23.2,22.7,23.1,23.26,22.86,23.06,22.66,23.06,23.16,23.08,22.64,23.02,23.06,23.04,22.98,23.28,23.4,23.44,23.28,22.44,23.08,23.02

val_acc1
20.4,22.5,22.4,22.8,23.0,22.5,23.2,22.7,22.5,22.2,23.0,23.9,22.8,23.2,22.5,24.3,23.4,24.1,23.0,23.5,22.8,23.4,23.5,23.9,22.8,23.1,23.7,23.0,23.4,23.1,23.2,23.1,23.4,23.2,23.0,22.3,22.5,22.4,23.9,23.8,23.5,23.9,23.0,23.9,23.0,23.0,22.9,23.2,22.6,23.0,22.8,23.6,22.9,23.6,23.7,23.9,23.3,23.5,22.8,22.2,23.4,23.4,23.5,23.6,23.6,23.0,23.6,22.9,22.8,23.4,23.9,22.4,23.1,23.0,22.9,22.8,23.5,23.6,22.4,24.1,23.9,22.7,23.4,22.9,22.6,23.2,23.4,23.4,22.5,24.0,23.2,22.5,22.9,23.6,23.0,22.1,23.2,23.4,22.5,23.2

val_loss
2.1,2.051,2.043,2.04,2.038,2.033,2.033,2.028,2.034,2.034,2.03,2.036,2.034,2.029,2.026,2.02,2.027,2.024,2.021,2.017,2.026,2.025,2.013,2.013,2.023,2.027,2.022,2.027,2.026,2.022,2.022,2.02,2.021,2.029,2.022,2.03,2.03,2.031,2.018,2.024,2.02,2.017,2.017,2.031,2.02,2.019,2.024,2.022,2.017,2.024,2.026,2.022,2.021,2.022,2.017,2.014,2.015,2.023,2.024,2.023,2.017,2.03,2.024,2.017,2.022,2.021,2.021,2.012,2.021,2.02,2.019,2.021,2.017,2.024,2.024,2.018,2.02,2.02,2.016,2.021,2.019,2.027,2.014,2.021,2.024,2.019,2.017,2.018,2.017,2.017,2.016,2.022,2.017,2.018,2.02,2.028,2.015,2.015,2.02,2.023

learning_rate
0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001

