parameters
adagrad_lr:0.001
adagrad_learning_decay:0.99
adagrad_weight_decay:0.0
adagrad_initial_acumulator:0.0
architecture:vgg19bn_
dataset:cifar10
combine_datasets:True
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:0.2
executions:1
base_seed:1230
training_method:adagrad
learning_method:constant

train_loss
2.171,2.116,2.107,2.1,2.097,2.094,2.092,2.092,2.092,2.089,2.09,2.09,2.089,2.087,2.088,2.086,2.088,2.086,2.088,2.084,2.087,2.085,2.084,2.085,2.083,2.086,2.089,2.083,2.084,2.08,2.084,2.084,2.084,2.081,2.084,2.085,2.085,2.083,2.084,2.083,2.082,2.083,2.085,2.08,2.082,2.082,2.083,2.084,2.082,2.084,2.083,2.08,2.083,2.08,2.082,2.079,2.082,2.083,2.084,2.081,2.082,2.082,2.083,2.082,2.081,2.08,2.083,2.084,2.08,2.081,2.082,2.082,2.081,2.079,2.079,2.079,2.081,2.081,2.082,2.08,2.08,2.081,2.08,2.081,2.084,2.081,2.08,2.081,2.083,2.081,2.082,2.08,2.08,2.08,2.081,2.079,2.08,2.079,2.079,2.08

train_acc1
19.45,21.87,22.31,22.58,22.82,22.44,22.45,22.86,22.54,22.77,22.9,22.84,22.95,22.79,22.98,22.94,23.08,23.18,23.17,22.96,23.09,23.19,23.09,22.97,23.3,22.74,22.69,22.91,23.21,23.02,22.66,22.9,23.31,22.93,23.2,22.98,23.04,23.04,22.93,22.82,23.13,23.07,22.81,22.85,23.19,23.08,23.08,23.12,23.2,23.06,23.2,23.37,23.11,23.04,23.02,23.54,23.39,23.14,22.81,23.02,22.91,23.21,23.0,23.5,23.28,22.97,22.84,22.95,23.15,23.04,23.1,22.75,23.16,23.11,22.92,23.37,23.24,23.0,23.11,23.06,22.93,23.21,23.36,23.28,22.85,22.81,22.78,23.33,22.9,23.1,23.24,22.91,23.02,23.08,23.03,22.81,22.93,22.91,22.96,23.73

val_acc1
21.95,21.85,22.85,22.2,23.25,22.65,22.65,22.95,23.4,22.45,22.45,23.25,21.6,22.45,22.35,22.4,23.1,23.3,23.25,22.95,22.9,23.05,23.0,22.75,23.4,23.3,22.5,23.85,23.75,22.2,23.3,23.0,23.35,22.95,22.85,22.65,22.6,23.15,22.6,22.4,24.05,23.55,22.95,23.6,23.35,23.25,23.1,22.95,22.9,23.0,22.65,22.75,22.95,22.65,22.9,23.45,22.6,23.4,23.45,23.3,23.35,23.75,23.6,22.85,23.75,23.3,23.05,22.95,24.15,23.65,23.55,22.45,23.1,22.8,23.4,23.15,23.0,23.15,22.5,22.6,22.85,23.1,22.5,23.3,21.75,23.2,23.15,22.85,23.55,22.5,23.15,22.7,23.2,23.0,22.8,23.15,23.95,23.6,23.25,24.0

val_loss
2.128,2.115,2.106,2.104,2.096,2.1,2.096,2.097,2.101,2.099,2.099,2.093,2.091,2.098,2.088,2.095,2.09,2.09,2.093,2.095,2.091,2.088,2.087,2.091,2.09,2.089,2.085,2.09,2.089,2.092,2.09,2.088,2.087,2.088,2.089,2.087,2.084,2.088,2.087,2.094,2.086,2.086,2.086,2.088,2.086,2.088,2.086,2.085,2.083,2.091,2.086,2.085,2.088,2.084,2.083,2.09,2.089,2.089,2.086,2.088,2.087,2.089,2.084,2.086,2.09,2.089,2.082,2.087,2.086,2.088,2.084,2.088,2.091,2.086,2.088,2.088,2.087,2.086,2.086,2.087,2.086,2.087,2.087,2.086,2.085,2.08,2.087,2.085,2.081,2.082,2.083,2.086,2.082,2.083,2.088,2.082,2.083,2.08,2.086,2.085

learning_rate
0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001

