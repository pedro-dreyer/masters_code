parameters
adagrad_lr:0.001
adagrad_learning_decay:0.99
adagrad_weight_decay:0.0
adagrad_initial_acumulator:0.0
architecture:vgg19bn_
dataset:cifar10
combine_datasets:True
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:0.6
executions:1
base_seed:1230
training_method:adagrad
learning_method:constant

train_loss
2.099,2.066,2.059,2.057,2.055,2.055,2.054,2.053,2.052,2.051,2.051,2.051,2.051,2.05,2.05,2.05,2.049,2.049,2.05,2.049,2.049,2.049,2.047,2.049,2.048,2.049,2.049,2.048,2.049,2.048,2.047,2.047,2.049,2.048,2.048,2.047,2.048,2.048,2.047,2.047,2.048,2.047,2.048,2.047,2.047,2.047,2.047,2.047,2.047,2.048,2.047,2.047,2.046,2.047,2.047,2.046,2.046,2.047,2.047,2.046,2.047,2.047,2.046,2.046,2.045,2.047,2.045,2.046,2.046,2.047,2.047,2.046,2.047,2.046,2.045,2.047,2.047,2.047,2.047,2.046,2.047,2.046,2.046,2.046,2.047,2.046,2.046,2.048,2.047,2.046,2.046,2.046,2.046,2.046,2.046,2.046,2.046,2.046,2.045,2.047

train_acc1
19.837,20.917,21.16,21.303,21.33,21.31,21.48,21.467,21.7,21.503,21.887,21.627,21.69,21.813,21.653,21.717,21.687,21.813,21.643,22.023,21.793,21.673,21.997,21.663,21.747,21.693,21.607,21.79,21.833,21.763,21.893,21.793,21.54,21.733,21.72,21.933,21.937,21.713,21.863,21.813,21.713,21.92,21.847,21.817,21.947,21.973,21.66,21.75,21.857,21.7,21.863,21.947,21.863,21.7,21.83,21.977,21.917,21.987,21.763,21.743,22.023,21.787,21.837,21.777,21.78,21.887,21.85,22.063,21.933,22.03,21.787,21.81,21.84,21.8,21.853,21.693,22.027,21.767,22.02,21.98,21.88,21.93,21.677,22.17,21.753,21.72,21.913,21.73,21.883,21.947,22.01,22.003,21.86,21.847,21.937,22.077,21.763,21.843,21.96,21.807

val_acc1
21.433,21.4,21.467,21.983,21.833,22.483,21.867,22.35,21.95,22.217,22.067,22.85,22.033,22.317,21.933,21.85,22.083,22.117,22.083,21.75,22.217,22.45,22.317,22.3,22.4,22.217,22.25,22.567,22.317,22.417,22.367,22.517,22.45,22.317,22.017,21.817,22.317,21.95,22.217,22.283,22.55,22.267,22.367,22.433,22.567,22.317,22.217,22.217,22.55,22.333,21.967,22.267,22.4,21.817,22.583,22.75,23.0,22.117,22.017,22.833,21.967,22.417,22.567,22.35,22.083,22.1,22.15,21.883,22.517,22.0,22.517,22.55,22.333,22.65,21.85,22.517,22.067,22.2,21.75,22.65,22.333,22.167,21.7,22.15,21.967,22.2,22.717,22.517,22.083,22.25,22.367,22.417,22.683,22.3,22.45,22.633,22.25,22.817,22.017,22.283

val_loss
2.063,2.05,2.051,2.048,2.047,2.046,2.046,2.045,2.046,2.04,2.044,2.043,2.042,2.042,2.04,2.042,2.038,2.043,2.036,2.04,2.038,2.04,2.038,2.04,2.042,2.039,2.04,2.04,2.041,2.039,2.039,2.041,2.039,2.041,2.036,2.039,2.038,2.039,2.041,2.036,2.037,2.039,2.038,2.042,2.04,2.035,2.038,2.039,2.036,2.036,2.038,2.036,2.04,2.041,2.036,2.035,2.038,2.038,2.039,2.036,2.038,2.039,2.037,2.037,2.037,2.039,2.04,2.036,2.04,2.038,2.035,2.039,2.037,2.038,2.04,2.038,2.038,2.039,2.034,2.036,2.036,2.037,2.036,2.039,2.037,2.036,2.037,2.037,2.037,2.04,2.036,2.034,2.037,2.038,2.034,2.037,2.036,2.038,2.036,2.036

learning_rate
0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001

