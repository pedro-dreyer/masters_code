parameters
adagrad_lr:0.001
adagrad_learning_decay:0.99
adagrad_weight_decay:0.0
adagrad_initial_acumulator:0.0
architecture:vgg19bn_
dataset:cifar10
combine_datasets:True
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:0.4
executions:1
base_seed:1230
training_method:adagrad
learning_method:constant

train_loss
2.088,2.047,2.04,2.039,2.035,2.035,2.033,2.032,2.033,2.031,2.03,2.03,2.03,2.03,2.029,2.031,2.028,2.028,2.028,2.029,2.028,2.027,2.028,2.027,2.028,2.029,2.027,2.029,2.028,2.027,2.027,2.027,2.026,2.027,2.028,2.028,2.026,2.027,2.024,2.026,2.026,2.026,2.027,2.027,2.027,2.027,2.025,2.025,2.026,2.026,2.027,2.027,2.025,2.026,2.026,2.025,2.026,2.026,2.026,2.026,2.026,2.027,2.024,2.026,2.027,2.026,2.026,2.026,2.024,2.024,2.026,2.026,2.025,2.027,2.026,2.025,2.026,2.025,2.026,2.027,2.026,2.025,2.024,2.025,2.025,2.025,2.025,2.027,2.024,2.025,2.026,2.025,2.024,2.026,2.025,2.026,2.024,2.025,2.026,2.025

train_acc1
20.98,22.32,22.635,22.39,22.435,22.54,22.52,22.5,22.52,22.605,22.65,22.82,22.54,22.575,22.57,22.61,22.475,22.58,22.695,22.545,22.72,22.63,22.825,22.445,22.66,22.465,22.795,22.605,22.7,22.755,22.605,22.885,22.665,22.58,22.535,22.955,22.875,22.63,22.975,22.745,22.62,22.485,22.53,22.425,22.66,22.62,22.515,22.625,22.535,22.905,22.71,22.75,22.535,22.635,22.53,22.59,22.73,22.585,22.715,22.555,22.505,22.64,22.92,22.775,22.59,22.685,22.67,22.8,22.82,22.82,22.835,22.745,22.7,22.57,22.595,22.55,22.76,22.855,22.495,22.65,22.515,22.63,22.765,22.87,22.84,22.48,22.85,22.565,22.61,22.66,22.625,22.61,22.655,22.78,22.535,22.53,22.47,22.795,22.685,22.565

val_acc1
23.575,23.225,23.35,23.775,23.95,23.575,24.025,23.45,24.025,23.325,23.2,23.825,23.45,23.525,23.6,23.75,23.85,23.6,23.4,23.675,23.925,23.75,24.025,23.45,22.975,23.85,23.5,23.675,23.025,23.9,23.45,23.45,23.325,23.975,23.95,24.325,23.325,23.4,23.1,23.825,23.525,23.85,23.675,23.575,23.575,23.425,23.8,23.575,23.225,23.8,23.8,23.7,23.8,23.35,24.05,23.525,23.875,23.575,23.6,23.425,23.75,23.625,23.975,23.5,23.825,23.125,24.175,23.475,23.7,23.45,24.3,23.7,23.625,23.95,23.425,23.775,24.15,23.7,24.025,23.925,23.625,24.2,24.0,23.475,23.775,23.725,23.3,23.6,23.7,23.675,23.425,24.075,23.825,23.65,23.7,23.725,23.675,23.925,23.7,23.975

val_loss
2.056,2.045,2.041,2.041,2.036,2.032,2.032,2.033,2.034,2.032,2.034,2.033,2.03,2.032,2.032,2.026,2.031,2.026,2.032,2.031,2.033,2.028,2.027,2.031,2.031,2.026,2.026,2.028,2.031,2.026,2.025,2.029,2.029,2.028,2.028,2.025,2.026,2.026,2.027,2.027,2.026,2.023,2.03,2.03,2.03,2.027,2.032,2.027,2.033,2.026,2.025,2.03,2.025,2.025,2.027,2.023,2.025,2.025,2.028,2.028,2.024,2.028,2.026,2.024,2.026,2.025,2.024,2.025,2.028,2.028,2.026,2.022,2.027,2.023,2.023,2.023,2.023,2.025,2.028,2.025,2.028,2.029,2.027,2.024,2.029,2.029,2.027,2.022,2.029,2.029,2.027,2.022,2.028,2.026,2.025,2.025,2.024,2.023,2.026,2.026

learning_rate
0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001

