parameters
adam_lr:0.001
adam_beta1:0.9
adam_beta2:0.999
adam_eps:1e-08
adam_weight_decay:0.0
adam_amsgrad:False
architecture:vgg19bn_
dataset:cifar10
combine_datasets:True
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:0.1
executions:1
base_seed:1230
training_method:adam
learning_method:constant

train_loss
2.044,1.968,1.885,1.865,1.853,1.859,1.8,1.74,1.816,1.702,1.681,1.636,1.632,1.598,1.621,1.553,1.54,1.448,1.418,1.431,1.469,1.551,1.419,1.439,1.44,1.311,1.28,1.275,1.264,1.178,1.187,1.218,1.225,1.229,1.146,1.153,1.054,1.061,1.014,0.975,0.941,0.967,0.914,0.929,0.858,0.786,0.87,1.03,0.85,0.773,0.717,0.751,0.686,0.775,0.822,0.637,0.746,0.677,0.57,0.566,0.676,0.577,0.517,0.674,0.545,0.486,0.421,0.443,0.47,0.568,0.532,0.466,0.467,0.429,0.395,0.388,0.554,0.51,0.549,0.393,0.449,0.375,0.366,0.339,0.374,0.48,0.455,0.311,0.259,0.326,0.444,0.423,0.309,0.237,0.224,0.289,0.206,0.262,0.321,0.237

train_acc1
17.76,18.38,21.24,22.6,24.44,24.76,26.8,29.12,27.06,31.82,31.54,34.16,33.86,34.72,33.18,36.36,37.12,39.42,41.9,41.68,41.0,38.96,42.16,42.02,43.68,48.4,48.7,48.58,49.6,52.92,54.1,55.18,52.92,53.82,56.14,56.36,60.42,61.02,62.92,64.16,66.04,64.7,67.88,65.44,68.7,72.24,71.0,65.02,71.22,72.7,73.98,72.92,75.56,76.06,71.52,78.58,73.58,75.58,79.8,79.88,76.82,79.24,82.5,78.12,81.46,82.8,85.54,84.06,84.32,82.22,84.7,83.92,84.02,85.46,86.1,88.66,80.96,85.08,81.34,87.24,85.42,87.62,88.38,88.46,88.78,84.32,85.68,89.86,91.8,90.04,87.98,87.26,89.92,92.02,94.1,90.84,93.54,91.36,90.16,93.2

val_acc1
17.6,19.8,19.4,21.6,23.8,23.9,26.0,25.6,28.5,33.1,29.3,33.3,33.1,34.5,31.1,35.3,34.3,35.4,39.0,37.9,36.7,39.0,40.8,39.9,42.5,43.3,45.8,46.1,46.3,48.9,52.7,48.5,54.1,52.1,51.0,49.5,54.8,57.4,54.4,58.2,52.3,58.9,55.4,59.8,63.0,60.5,49.2,59.0,64.2,59.6,64.8,64.2,64.7,64.7,65.5,63.7,63.8,64.5,66.8,61.6,65.8,64.0,60.5,66.0,66.1,66.5,68.0,66.0,66.0,64.9,70.1,68.0,67.7,67.8,70.5,63.0,64.6,70.9,67.6,69.5,67.8,68.1,68.1,69.3,67.5,65.1,71.8,72.0,71.1,67.9,67.3,71.1,70.9,71.1,68.9,69.7,70.5,70.8,72.4,68.8

val_loss
2.905,2.019,1.911,2.274,2.104,1.898,1.874,1.902,1.808,1.746,1.806,1.806,1.692,1.682,1.662,1.64,1.648,1.619,1.535,1.774,1.738,1.602,1.529,1.745,1.446,1.469,1.463,1.489,1.383,1.398,1.337,1.397,1.301,1.301,1.375,1.396,1.317,1.287,1.327,1.258,1.386,1.281,1.335,1.184,1.143,1.256,1.841,1.297,1.082,1.283,1.142,1.185,1.163,1.201,1.045,1.176,1.207,1.138,1.123,1.34,1.179,1.151,1.353,1.172,1.099,1.103,1.052,1.177,1.176,1.144,1.083,1.147,1.109,1.228,1.038,1.461,1.22,0.996,1.047,1.081,1.2,1.16,1.273,1.139,1.197,1.203,1.015,1.091,1.12,1.199,1.211,1.047,1.126,1.095,1.247,1.153,1.215,1.182,1.192,1.358

learning_rate
0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001

