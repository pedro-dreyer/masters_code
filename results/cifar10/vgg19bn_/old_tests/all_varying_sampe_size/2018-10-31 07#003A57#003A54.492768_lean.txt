parameters
adam_lr:0.001
adam_beta1:0.9
adam_beta2:0.999
adam_eps:1e-08
adam_weight_decay:0.0
adam_amsgrad:False
architecture:vgg19bn_
dataset:cifar10
combine_datasets:True
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:0.6
executions:1
base_seed:1230
training_method:adam
learning_method:constant

train_loss
1.854,1.492,1.264,1.063,0.925,0.839,0.759,0.709,0.643,0.592,0.555,0.522,0.479,0.462,0.421,0.397,0.38,0.351,0.344,0.316,0.303,0.286,0.266,0.255,0.249,0.235,0.22,0.202,0.2,0.191,0.183,0.174,0.17,0.163,0.153,0.14,0.132,0.138,0.133,0.124,0.117,0.109,0.115,0.101,0.103,0.095,0.101,0.092,0.089,0.088,0.087,0.08,0.072,0.072,0.075,0.069,0.07,0.064,0.063,0.062,0.065,0.061,0.063,0.057,0.055,0.053,0.052,0.049,0.058,0.055,0.045,0.045,0.05,0.048,0.042,0.046,0.04,0.04,0.043,0.036,0.041,0.039,0.045,0.038,0.041,0.038,0.033,0.033,0.037,0.041,0.035,0.035,0.033,0.03,0.034,0.031,0.033,0.034,0.031,0.033

train_acc1
26.143,42.29,53.79,62.027,67.17,71.023,74.007,76.093,78.037,80.42,81.637,82.673,84.24,84.853,86.127,87.003,87.497,88.427,88.927,89.773,90.177,90.703,91.523,91.757,91.773,92.397,92.97,93.477,93.72,93.887,94.05,94.323,94.56,94.81,95.1,95.58,95.823,95.707,95.78,96.147,96.46,96.547,96.387,96.81,96.81,97.05,96.8,97.193,97.287,97.26,97.203,97.523,97.747,97.823,97.613,97.82,97.78,97.917,98.003,98.023,97.943,98.06,97.983,98.263,98.303,98.343,98.307,98.613,98.213,98.207,98.603,98.577,98.42,98.52,98.71,98.58,98.683,98.673,98.707,98.897,98.747,98.797,98.62,98.887,98.733,98.72,98.983,99.003,98.87,98.737,98.907,98.947,98.977,99.02,98.92,99.087,98.993,98.977,99.007,98.96

val_acc1
34.283,39.25,51.183,59.7,65.933,65.333,73.333,74.317,75.15,78.883,79.417,80.55,79.883,79.633,81.983,82.05,80.383,82.333,82.583,81.867,84.533,84.333,84.967,85.617,82.917,84.567,84.7,84.85,83.217,84.95,85.083,83.967,86.433,85.467,85.617,86.2,85.3,85.9,84.933,85.7,85.883,86.417,86.817,86.0,86.533,86.55,87.033,86.983,86.3,87.55,87.117,86.7,87.1,86.417,85.917,86.233,86.667,87.117,87.3,87.633,87.133,86.917,87.05,87.117,87.067,87.067,87.917,87.25,86.35,86.917,87.267,87.317,87.35,87.417,86.833,87.2,87.0,87.85,87.35,86.467,86.417,87.217,87.133,87.6,86.817,87.5,87.8,87.2,86.4,87.3,87.2,87.617,87.95,86.967,87.417,87.7,87.65,87.6,87.783,87.867

val_loss
1.754,1.689,1.324,1.139,0.981,1.053,0.773,0.774,0.731,0.674,0.645,0.611,0.641,0.618,0.558,0.592,0.652,0.567,0.551,0.596,0.515,0.505,0.506,0.506,0.582,0.528,0.51,0.554,0.593,0.528,0.564,0.606,0.497,0.553,0.545,0.529,0.579,0.542,0.569,0.534,0.588,0.544,0.534,0.572,0.521,0.566,0.542,0.516,0.574,0.509,0.504,0.562,0.554,0.585,0.614,0.603,0.575,0.545,0.55,0.575,0.595,0.575,0.559,0.572,0.552,0.594,0.53,0.589,0.634,0.612,0.586,0.624,0.587,0.543,0.567,0.536,0.623,0.596,0.571,0.658,0.634,0.584,0.6,0.607,0.595,0.609,0.58,0.636,0.641,0.605,0.662,0.627,0.637,0.674,0.684,0.619,0.637,0.578,0.631,0.593

learning_rate
0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001

