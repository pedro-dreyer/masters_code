parameters
adam_beta1:0.9
adam_beta2:0.999
adam_eps:1e-08
adam_weight_decay:0.0
adam_amsgrad:False
architecture:vgg19bn_
dataset:cifar10
combine_datasets:False
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:0.8
initial_learning_rate:0.01
executions:1
base_seed:1230
training_method:adam
learning_method:constant

train_loss
2.238,1.87,1.732,1.491,1.232,1.011,0.875,0.78,0.699,0.634,0.587,0.553,0.508,0.479,0.45,0.423,0.403,0.381,0.364,0.345,0.334,0.309,0.297,0.284,0.268,0.26,0.248,0.241,0.223,0.217,0.212,0.201,0.194,0.181,0.175,0.18,0.163,0.161,0.161,0.149,0.143,0.141,0.132,0.129,0.125,0.124,0.119,0.118,0.111,0.105,0.104,0.103,0.101,0.098,0.092,0.092,0.093,0.092,0.086,0.079,0.08,0.085,0.081,0.079,0.075,0.074,0.069,0.072,0.067,0.073,0.067,0.063,0.064,0.061,0.064,0.063,0.058,0.064,0.056,0.061,0.057,0.053,0.054,0.057,0.055,0.049,0.048,0.048,0.049,0.052,0.048,0.047,0.05,0.047,0.042,0.045,0.045,0.049,0.046,0.049

train_acc1
15.105,26.158,32.332,41.455,54.56,63.57,68.83,72.805,76.192,78.855,80.308,81.598,83.032,84.052,85.205,86.09,86.832,87.542,88.172,88.54,89.075,89.89,90.258,90.712,91.335,91.505,91.795,92.21,92.652,92.985,93.142,93.39,93.688,94.075,94.278,94.198,94.692,94.79,94.805,95.208,95.388,95.41,95.68,95.722,95.925,96.045,96.152,96.16,96.48,96.565,96.678,96.708,96.75,96.788,97.005,97.052,96.995,97.022,97.178,97.505,97.383,97.26,97.362,97.508,97.575,97.65,97.76,97.693,97.922,97.755,97.815,98.012,97.972,98.115,97.935,98.0,98.118,98.0,98.212,98.078,98.27,98.28,98.25,98.275,98.232,98.408,98.498,98.485,98.48,98.292,98.49,98.502,98.475,98.492,98.682,98.578,98.558,98.47,98.552,98.468

val_acc1
22.44,24.78,34.76,44.18,56.91,45.5,68.8,67.58,73.95,78.46,75.5,71.28,77.18,79.36,79.93,81.79,81.98,82.92,82.87,85.03,84.21,82.86,85.94,85.2,85.04,83.55,86.0,85.95,83.22,84.51,85.87,85.6,86.24,87.47,82.9,87.53,87.36,86.94,86.54,87.66,87.93,87.09,87.1,87.19,87.97,87.17,86.75,87.46,87.31,87.02,87.05,87.4,88.32,87.64,87.81,87.9,87.88,87.77,87.95,88.49,88.72,87.89,87.58,88.66,87.33,88.14,86.77,87.83,88.64,88.49,86.54,87.95,88.28,88.12,87.64,88.05,88.33,88.98,88.3,87.84,88.32,89.22,88.43,88.25,88.65,88.12,88.14,88.39,88.76,88.51,87.7,87.74,86.73,88.34,89.1,88.56,89.11,89.06,87.48,89.09

val_loss
1.932,1.894,1.711,1.52,1.222,1.71,0.901,0.96,0.818,0.658,0.791,0.973,0.744,0.682,0.649,0.597,0.558,0.554,0.568,0.484,0.507,0.598,0.468,0.51,0.513,0.578,0.475,0.485,0.639,0.574,0.487,0.51,0.483,0.485,0.663,0.442,0.454,0.466,0.495,0.496,0.479,0.465,0.501,0.524,0.469,0.513,0.562,0.472,0.516,0.54,0.54,0.535,0.476,0.493,0.497,0.484,0.479,0.523,0.533,0.544,0.486,0.503,0.563,0.497,0.572,0.511,0.588,0.541,0.509,0.489,0.635,0.54,0.553,0.534,0.584,0.561,0.533,0.498,0.546,0.575,0.508,0.502,0.517,0.569,0.512,0.577,0.544,0.579,0.514,0.542,0.637,0.602,0.645,0.53,0.548,0.585,0.529,0.509,0.603,0.516

learning_rate
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01

