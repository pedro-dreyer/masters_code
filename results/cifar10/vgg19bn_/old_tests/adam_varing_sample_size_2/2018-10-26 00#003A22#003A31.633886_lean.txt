parameters
adam_beta1:0.9
adam_beta2:0.999
adam_eps:1e-08
adam_weight_decay:0.0
adam_amsgrad:False
architecture:vgg19bn_
dataset:cifar10
combine_datasets:False
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:0.4
initial_learning_rate:0.001
executions:1
base_seed:1230
training_method:adam
learning_method:constant

train_loss
1.938,1.769,1.623,1.455,1.359,1.237,1.141,1.062,0.995,0.952,0.876,0.825,0.754,0.717,0.647,0.606,0.59,0.541,0.514,0.483,0.449,0.441,0.412,0.393,0.368,0.361,0.334,0.324,0.308,0.291,0.276,0.263,0.257,0.228,0.232,0.221,0.207,0.198,0.179,0.184,0.179,0.161,0.16,0.145,0.149,0.139,0.126,0.126,0.131,0.121,0.127,0.111,0.105,0.108,0.098,0.095,0.101,0.087,0.084,0.085,0.079,0.083,0.086,0.086,0.079,0.076,0.068,0.067,0.068,0.074,0.064,0.076,0.057,0.057,0.057,0.058,0.054,0.058,0.058,0.056,0.055,0.05,0.055,0.052,0.045,0.048,0.051,0.046,0.045,0.046,0.059,0.049,0.045,0.036,0.048,0.041,0.037,0.044,0.043,0.037

train_acc1
21.04,27.595,34.31,41.515,47.82,53.64,58.325,61.695,64.565,66.955,69.61,72.205,73.92,75.65,77.9,79.66,80.13,81.48,82.715,83.84,85.29,85.35,86.195,86.915,87.845,87.965,88.9,89.35,89.965,90.425,90.915,91.575,91.66,92.375,92.375,92.5,93.185,93.365,94.04,93.99,94.09,94.605,94.715,95.36,95.175,95.375,95.705,95.81,95.715,96.015,95.89,96.39,96.705,96.555,96.755,96.835,96.525,97.175,97.33,97.26,97.535,97.47,97.22,97.3,97.415,97.62,97.78,97.84,97.755,97.535,97.95,97.705,98.165,98.155,98.14,98.07,98.355,98.105,98.045,98.27,98.235,98.325,98.265,98.335,98.66,98.44,98.42,98.53,98.595,98.605,98.125,98.515,98.545,98.905,98.46,98.735,98.765,98.64,98.55,98.85

val_acc1
23.21,31.15,33.76,45.69,45.4,52.11,57.01,62.04,60.97,63.19,66.93,67.54,66.8,71.62,74.59,75.13,75.34,77.75,75.19,76.16,79.13,78.15,79.56,79.88,81.31,80.21,80.51,82.19,78.46,81.39,82.64,82.83,82.38,81.64,84.01,83.61,82.74,83.4,84.2,83.34,83.44,84.47,83.12,82.51,83.42,83.6,82.33,83.34,83.58,83.33,84.41,84.1,84.43,84.14,83.53,84.79,82.89,85.66,83.62,84.15,84.27,84.08,84.37,82.79,84.58,83.25,84.87,85.01,84.99,84.82,84.39,85.28,85.16,85.38,85.12,85.05,85.28,85.54,84.67,85.85,85.07,85.33,85.03,85.93,85.15,85.94,85.69,85.74,84.96,85.42,85.41,85.57,85.02,84.72,85.58,85.23,86.14,85.04,84.84,85.77

val_loss
2.114,1.653,1.857,1.349,1.487,1.299,1.192,1.023,1.125,1.117,0.939,0.957,1.094,0.875,0.757,0.808,0.776,0.688,0.823,0.764,0.668,0.731,0.657,0.668,0.607,0.688,0.667,0.585,0.782,0.644,0.626,0.602,0.617,0.671,0.571,0.611,0.616,0.612,0.584,0.651,0.638,0.603,0.692,0.759,0.645,0.665,0.751,0.653,0.693,0.699,0.676,0.666,0.658,0.681,0.701,0.649,0.712,0.627,0.754,0.756,0.702,0.745,0.73,0.816,0.704,0.818,0.697,0.682,0.736,0.689,0.736,0.7,0.728,0.695,0.666,0.681,0.739,0.676,0.768,0.714,0.751,0.748,0.717,0.703,0.755,0.666,0.688,0.685,0.723,0.731,0.714,0.7,0.706,0.768,0.708,0.726,0.716,0.747,0.809,0.743

learning_rate
0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001

