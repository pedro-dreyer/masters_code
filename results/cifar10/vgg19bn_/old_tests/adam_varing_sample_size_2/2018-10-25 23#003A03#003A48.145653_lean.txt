parameters
adam_beta1:0.9
adam_beta2:0.999
adam_eps:1e-08
adam_weight_decay:0.0
adam_amsgrad:False
architecture:vgg19bn_
dataset:cifar10
combine_datasets:False
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:0.8
initial_learning_rate:0.001
executions:1
base_seed:1230
training_method:adam
learning_method:constant

train_loss
1.85,1.488,1.237,1.07,0.917,0.807,0.724,0.667,0.603,0.562,0.52,0.489,0.454,0.426,0.398,0.38,0.358,0.342,0.317,0.302,0.286,0.263,0.254,0.239,0.224,0.213,0.205,0.195,0.184,0.173,0.169,0.157,0.151,0.149,0.14,0.136,0.127,0.118,0.114,0.104,0.108,0.103,0.102,0.099,0.086,0.09,0.079,0.085,0.08,0.078,0.071,0.065,0.07,0.063,0.069,0.061,0.06,0.054,0.058,0.062,0.055,0.054,0.053,0.056,0.052,0.05,0.05,0.047,0.05,0.044,0.045,0.043,0.039,0.039,0.044,0.041,0.043,0.037,0.044,0.035,0.035,0.036,0.038,0.034,0.033,0.035,0.034,0.032,0.035,0.036,0.034,0.03,0.037,0.027,0.031,0.028,0.028,0.034,0.032,0.027

train_acc1
25.952,41.607,54.473,61.952,67.815,71.88,74.95,77.133,79.9,81.232,82.798,83.557,85.075,86.048,86.925,87.475,88.33,88.83,89.625,90.188,90.678,91.295,91.73,92.185,92.658,93.01,93.318,93.608,93.932,94.235,94.472,94.87,95.048,95.198,95.405,95.65,95.642,96.155,96.145,96.642,96.452,96.665,96.645,96.805,97.265,97.14,97.38,97.165,97.342,97.49,97.602,97.955,97.693,97.862,97.688,98.085,98.208,98.305,98.158,98.052,98.182,98.255,98.23,98.167,98.398,98.345,98.34,98.542,98.365,98.56,98.485,98.572,98.718,98.755,98.595,98.718,98.645,98.83,98.57,98.88,98.803,98.76,98.805,98.918,98.965,98.96,98.915,98.948,98.862,98.82,98.935,99.09,98.825,99.1,98.922,99.165,99.048,98.932,98.918,99.142

val_acc1
25.94,49.14,58.49,65.51,69.39,67.41,76.58,77.52,79.19,80.55,79.1,81.58,82.25,82.37,83.96,85.09,85.2,84.31,82.93,85.45,85.24,85.02,86.33,87.18,87.13,86.7,87.78,87.28,86.46,87.6,87.46,87.47,88.46,87.3,88.18,87.68,88.53,88.33,87.58,88.84,88.12,88.61,88.69,88.9,89.01,87.43,87.42,88.41,88.61,87.95,88.87,89.37,89.38,88.38,88.96,88.34,89.41,88.95,88.86,88.92,88.84,88.4,88.85,88.82,87.78,89.64,88.67,88.67,89.29,88.77,89.46,89.04,89.22,88.5,88.83,88.89,89.1,88.79,89.18,89.55,89.63,89.21,89.14,89.49,89.8,89.52,89.3,89.65,89.22,88.55,89.1,89.66,89.2,89.09,89.83,88.66,89.79,89.73,89.71,89.46

val_loss
2.006,1.377,1.151,0.965,0.896,0.961,0.695,0.665,0.646,0.608,0.662,0.577,0.559,0.551,0.513,0.47,0.454,0.504,0.578,0.463,0.49,0.509,0.452,0.435,0.433,0.458,0.408,0.442,0.485,0.436,0.449,0.462,0.416,0.492,0.418,0.482,0.421,0.439,0.501,0.466,0.459,0.441,0.447,0.441,0.483,0.558,0.55,0.494,0.478,0.477,0.479,0.464,0.462,0.488,0.473,0.523,0.457,0.481,0.521,0.492,0.51,0.512,0.508,0.499,0.58,0.439,0.527,0.536,0.489,0.496,0.507,0.482,0.529,0.566,0.54,0.574,0.542,0.545,0.511,0.505,0.477,0.502,0.5,0.529,0.479,0.526,0.52,0.552,0.494,0.59,0.497,0.554,0.49,0.559,0.505,0.622,0.527,0.515,0.515,0.512

learning_rate
0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001

