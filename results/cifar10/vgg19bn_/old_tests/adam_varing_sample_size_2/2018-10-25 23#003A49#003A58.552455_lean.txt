parameters
adam_beta1:0.9
adam_beta2:0.999
adam_eps:1e-08
adam_weight_decay:0.0
adam_amsgrad:False
architecture:vgg19bn_
dataset:cifar10
combine_datasets:False
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:0.6
initial_learning_rate:0.001
executions:1
base_seed:1230
training_method:adam
learning_method:constant

train_loss
1.878,1.66,1.403,1.205,1.075,0.992,0.929,0.844,0.774,0.706,0.641,0.582,0.543,0.503,0.473,0.452,0.422,0.394,0.372,0.348,0.33,0.314,0.296,0.279,0.271,0.257,0.243,0.229,0.211,0.202,0.198,0.188,0.175,0.167,0.164,0.149,0.147,0.142,0.13,0.126,0.122,0.117,0.114,0.104,0.101,0.102,0.1,0.094,0.086,0.087,0.084,0.083,0.083,0.072,0.073,0.071,0.069,0.073,0.069,0.066,0.06,0.06,0.06,0.062,0.062,0.055,0.057,0.051,0.049,0.054,0.05,0.05,0.055,0.046,0.044,0.041,0.046,0.045,0.051,0.044,0.04,0.044,0.036,0.038,0.044,0.039,0.036,0.037,0.036,0.038,0.036,0.038,0.041,0.031,0.036,0.039,0.028,0.028,0.033,0.033

train_acc1
23.533,33.413,45.537,55.093,61.33,65.067,67.64,71.107,73.623,76.277,78.56,80.463,82.033,83.167,84.403,85.147,85.98,86.927,87.537,88.243,89.09,89.757,90.137,90.81,91.143,91.727,92.043,92.533,93.09,93.197,93.31,93.727,94.267,94.51,94.61,95.027,95.257,95.333,95.733,95.933,96.023,96.063,96.223,96.58,96.663,96.693,96.583,96.883,97.193,97.14,97.263,97.257,97.293,97.64,97.577,97.563,97.793,97.653,97.767,97.857,98.11,97.987,98.033,97.987,98.083,98.163,98.12,98.327,98.433,98.22,98.447,98.38,98.2,98.503,98.637,98.677,98.523,98.537,98.387,98.517,98.727,98.6,98.817,98.81,98.643,98.78,98.847,98.747,98.833,98.87,98.803,98.84,98.65,99.0,98.783,98.767,99.103,99.06,98.953,98.98

val_acc1
28.76,35.87,46.57,58.3,51.96,59.85,68.88,69.4,71.15,76.42,75.51,73.92,78.49,78.78,80.77,81.59,82.67,82.89,83.36,83.55,83.21,83.92,84.75,85.08,84.35,84.48,84.71,85.47,84.65,86.03,86.38,85.22,86.44,86.5,86.68,84.66,86.92,86.11,87.33,87.36,85.88,86.31,87.14,86.68,87.38,87.06,86.98,86.81,87.57,87.51,87.73,87.1,87.71,87.06,87.91,88.06,88.31,88.02,86.7,86.81,86.55,88.12,87.84,87.34,87.67,88.19,87.88,87.78,87.66,88.38,87.36,87.81,87.48,87.85,88.13,87.98,88.29,87.65,87.59,88.0,87.58,87.58,87.75,87.72,88.9,87.74,88.61,88.25,87.67,88.11,88.15,86.97,88.26,87.78,88.03,87.81,88.19,87.75,87.72,88.22

val_loss
1.798,1.667,1.458,1.151,1.401,1.147,0.92,0.934,0.861,0.707,0.752,0.829,0.663,0.643,0.579,0.564,0.537,0.549,0.539,0.513,0.556,0.509,0.491,0.502,0.503,0.516,0.508,0.477,0.543,0.467,0.465,0.511,0.472,0.48,0.468,0.596,0.491,0.507,0.471,0.47,0.576,0.565,0.515,0.514,0.486,0.516,0.526,0.514,0.521,0.512,0.489,0.541,0.505,0.575,0.518,0.506,0.501,0.535,0.591,0.592,0.568,0.548,0.591,0.553,0.562,0.542,0.563,0.587,0.557,0.549,0.611,0.582,0.569,0.6,0.568,0.573,0.552,0.576,0.571,0.553,0.617,0.573,0.596,0.671,0.543,0.605,0.549,0.569,0.613,0.625,0.562,0.671,0.591,0.618,0.62,0.58,0.622,0.662,0.663,0.585

learning_rate
0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001

