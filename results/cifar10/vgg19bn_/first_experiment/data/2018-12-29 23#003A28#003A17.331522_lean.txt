parameters
adagrad_lr:0.1
adagrad_learning_decay:0.0
adagrad_weight_decay:0.0
adagrad_initial_acumulator:0.0
architecture:vgg19bn_
dataset:cifar10
combine_datasets:False
do_validation_set:False
epochs:100
batch_size:128
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:1.0
executions:2
base_seed:1230
training_method:adagrad
learning_method:constant

train_loss
2.591,2.282,2.188,1.916,1.813,1.703,1.559,1.419,1.295,1.166,1.046,0.95,0.867,0.801,0.738,0.694,0.643,0.605,0.57,0.538,0.508,0.49,0.466,0.444,0.425,0.405,0.389,0.373,0.352,0.337,0.324,0.31,0.302,0.291,0.28,0.266,0.255,0.24,0.234,0.225,0.223,0.205,0.199,0.198,0.186,0.178,0.171,0.164,0.158,0.153,0.146,0.139,0.133,0.133,0.126,0.121,0.111,0.113,0.107,0.103,0.1,0.094,0.094,0.087,0.083,0.082,0.079,0.076,0.075,0.07,0.068,0.069,0.063,0.057,0.063,0.055,0.056,0.054,0.05,0.048,0.048,0.051,0.048,0.046,0.046,0.04,0.04,0.038,0.038,0.038,0.037,0.031,0.033,0.033,0.032,0.031,0.03,0.029,0.029,0.025
2.531,2.032,1.848,1.735,1.597,1.451,1.308,1.153,1.02,0.917,0.829,0.746,0.682,0.631,0.586,0.553,0.514,0.49,0.46,0.432,0.408,0.391,0.37,0.351,0.338,0.32,0.303,0.289,0.273,0.26,0.251,0.24,0.228,0.219,0.209,0.197,0.188,0.175,0.17,0.164,0.155,0.148,0.14,0.137,0.128,0.121,0.115,0.113,0.103,0.102,0.1,0.092,0.089,0.083,0.082,0.079,0.069,0.071,0.069,0.063,0.06,0.062,0.055,0.056,0.053,0.053,0.05,0.045,0.046,0.042,0.045,0.038,0.039,0.037,0.037,0.032,0.038,0.033,0.031,0.031,0.032,0.032,0.028,0.029,0.028,0.026,0.023,0.024,0.024,0.024,0.026,0.024,0.024,0.021,0.022,0.021,0.019,0.023,0.019,0.017

train_acc1
10.71,11.842,14.892,23.354,27.282,31.91,38.398,45.35,51.21,57.19,61.75,65.752,68.884,72.006,74.372,76.316,77.938,79.574,80.77,81.964,82.936,83.554,84.326,85.182,85.84,86.58,87.032,87.638,88.22,88.938,89.24,89.744,89.978,90.468,90.722,91.3,91.616,91.96,92.202,92.638,92.646,93.066,93.514,93.37,93.844,94.05,94.31,94.58,94.748,94.904,95.188,95.458,95.566,95.634,95.916,95.976,96.234,96.238,96.46,96.562,96.796,96.842,96.868,97.16,97.242,97.288,97.398,97.442,97.558,97.674,97.814,97.714,97.95,98.084,97.986,98.226,98.14,98.24,98.354,98.374,98.396,98.346,98.404,98.532,98.536,98.648,98.626,98.75,98.746,98.754,98.794,98.99,98.914,98.924,98.89,98.936,99.028,99.058,99.03,99.206
12.234,19.616,26.292,31.59,38.28,45.004,51.372,58.248,64.032,67.86,71.192,74.022,76.534,78.368,79.982,81.222,82.462,83.526,84.466,85.554,86.124,86.892,87.506,88.126,88.712,89.086,89.712,90.164,90.798,91.202,91.526,91.792,92.334,92.578,92.904,93.346,93.58,93.88,94.22,94.364,94.678,94.994,95.332,95.378,95.632,95.902,96.09,96.046,96.552,96.576,96.596,96.872,96.992,97.154,97.204,97.356,97.676,97.624,97.646,97.862,97.966,97.964,98.106,98.118,98.262,98.246,98.31,98.494,98.432,98.616,98.506,98.698,98.664,98.718,98.76,98.938,98.724,98.914,98.912,98.968,98.892,98.958,99.042,99.018,99.062,99.11,99.16,99.208,99.178,99.22,99.112,99.16,99.22,99.256,99.232,99.262,99.352,99.22,99.334,99.402

val_acc1
11.74,12.08,19.16,23.94,31.81,37.51,39.71,44.21,47.54,56.61,60.36,56.27,66.97,67.87,72.51,73.44,77.1,72.22,79.47,79.96,80.04,80.79,80.95,81.99,83.49,82.13,83.69,83.97,83.95,84.11,83.86,84.87,83.82,85.72,82.59,85.3,85.91,85.97,85.65,86.25,85.96,86.83,87.32,86.32,87.3,87.75,87.56,86.99,87.2,87.06,87.46,87.21,87.58,87.5,81.1,86.78,87.38,87.81,87.95,87.5,87.93,87.17,88.14,88.16,88.44,87.17,87.45,88.28,88.81,88.67,87.69,88.26,88.6,87.53,88.32,88.48,88.73,88.64,87.84,88.45,88.87,88.82,88.9,88.53,88.65,89.04,88.57,88.34,88.53,88.34,88.47,88.68,88.92,88.85,88.04,89.27,89.08,89.35,87.68,88.85
15.31,23.29,28.18,31.39,37.47,47.88,53.41,51.96,60.42,61.29,66.25,74.33,72.71,72.56,78.67,81.32,80.09,76.96,82.55,83.07,79.22,84.2,83.12,81.18,84.97,85.32,85.26,85.47,86.17,86.43,85.67,84.93,86.09,86.83,85.4,87.35,87.48,86.78,87.68,86.97,87.28,87.1,87.49,87.64,88.64,88.15,87.73,87.64,88.06,88.01,87.58,87.8,87.93,88.06,86.11,88.62,87.9,88.6,87.39,88.65,88.07,88.58,88.41,88.52,88.04,88.44,88.72,88.03,88.25,88.25,88.14,88.63,88.58,88.21,88.79,88.38,88.43,88.75,88.64,88.56,88.82,88.81,88.6,86.97,88.91,88.97,88.73,89.11,88.78,89.4,88.48,89.24,88.6,89.24,88.72,88.97,88.23,89.03,88.88,88.8

val_loss
2.331,2.275,2.077,1.884,1.709,1.587,1.497,1.496,1.601,1.277,1.114,1.255,0.965,0.985,0.798,0.809,0.697,0.907,0.645,0.62,0.608,0.606,0.599,0.556,0.517,0.564,0.503,0.508,0.504,0.518,0.515,0.489,0.524,0.464,0.588,0.488,0.489,0.487,0.485,0.467,0.473,0.452,0.447,0.494,0.468,0.462,0.452,0.471,0.491,0.495,0.474,0.48,0.475,0.489,0.774,0.523,0.502,0.467,0.467,0.497,0.485,0.524,0.492,0.492,0.474,0.533,0.547,0.522,0.494,0.49,0.552,0.488,0.512,0.585,0.508,0.527,0.516,0.529,0.539,0.576,0.537,0.53,0.534,0.552,0.531,0.57,0.551,0.558,0.588,0.569,0.572,0.566,0.553,0.568,0.634,0.563,0.541,0.537,0.624,0.58
2.263,1.929,1.812,1.774,1.639,1.413,1.253,1.403,1.175,1.267,1.027,0.764,0.813,0.855,0.645,0.563,0.607,0.685,0.548,0.508,0.663,0.486,0.518,0.573,0.463,0.451,0.465,0.446,0.455,0.428,0.472,0.495,0.45,0.43,0.482,0.414,0.419,0.467,0.419,0.467,0.451,0.472,0.461,0.438,0.439,0.46,0.452,0.451,0.477,0.466,0.514,0.499,0.491,0.475,0.6,0.478,0.542,0.475,0.541,0.489,0.522,0.483,0.507,0.535,0.53,0.514,0.534,0.536,0.543,0.523,0.563,0.519,0.534,0.574,0.525,0.579,0.559,0.568,0.574,0.588,0.574,0.566,0.588,0.697,0.542,0.571,0.584,0.556,0.562,0.576,0.604,0.575,0.608,0.558,0.598,0.584,0.673,0.577,0.607,0.606

learning_rate
0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1
0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1

