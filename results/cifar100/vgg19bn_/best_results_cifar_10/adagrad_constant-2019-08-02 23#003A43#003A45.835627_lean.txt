parameters
adagrad_lr:0.05
adagrad_learning_decay:0.99
adagrad_weight_decay:0
adagrad_initial_acumulator:0
architecture:vgg19bn_
dataset:cifar100
combine_datasets:False
do_validation_set:False
epochs:100
batch_size:1024
test_set_split:0.1666667
validation_set_split:0.0
reduce_train_set:1
executions:2
base_seed:1230
training_method:adagrad
learning_method:constant

train_loss
5.685,4.908,4.858,4.867,4.837,4.861,4.813,4.852,4.806,4.825,4.812,4.806,4.82,4.818,4.797,4.811,4.796,4.792,4.817,4.778,4.801,4.794,4.796,4.805,4.794,4.783,4.779,4.778,4.769,4.784,4.766,4.798,4.793,4.775,4.786,4.786,4.77,4.813,4.773,4.783,4.766,4.777,4.789,4.772,4.769,4.765,4.775,4.774,4.787,4.777,4.808,4.775,4.788,4.768,4.775,4.782,4.777,4.775,4.796,4.784,4.758,4.777,4.767,4.773,4.763,4.793,4.768,4.782,4.76,4.768,4.772,4.776,4.759,4.768,4.783,4.763,4.759,4.774,4.78,4.776,4.754,4.783,4.769,4.776,4.782,4.767,4.777,4.776,4.788,4.779,4.774,4.782,4.777,4.766,4.766,4.763,4.752,4.76,4.798,4.777
5.516,4.787,4.747,4.737,4.716,4.717,4.707,4.7,4.701,4.701,4.695,4.697,4.678,4.697,4.688,4.695,4.679,4.681,4.684,4.684,4.683,4.678,4.68,4.676,4.687,4.678,4.681,4.679,4.673,4.676,4.684,4.68,4.679,4.672,4.674,4.68,4.672,4.676,4.67,4.676,4.67,4.679,4.671,4.671,4.676,4.676,4.676,4.668,4.673,4.67,4.672,4.677,4.684,4.668,4.665,4.674,4.672,4.674,4.668,4.68,4.672,4.673,4.665,4.663,4.662,4.663,4.672,4.667,4.667,4.668,4.658,4.666,4.672,4.671,4.666,4.672,4.667,4.663,4.673,4.667,4.675,4.67,4.664,4.673,4.673,4.669,4.663,4.669,4.661,4.67,4.671,4.666,4.667,4.673,4.669,4.667,4.659,4.665,4.668,4.667

train_acc1
0.974,1.13,0.936,0.948,0.958,0.948,0.9,0.854,0.82,0.86,0.852,0.878,0.918,0.896,0.92,0.954,0.886,0.874,0.954,0.858,0.868,0.838,0.87,0.9,0.85,0.844,0.872,0.854,0.88,0.908,0.904,0.874,0.868,0.878,0.944,0.864,0.878,0.894,0.852,0.852,0.838,0.912,0.864,0.834,0.902,0.882,0.846,0.896,0.898,0.832,0.884,0.84,0.912,0.826,0.866,0.868,0.88,0.868,0.858,0.886,0.902,0.874,0.866,0.872,0.89,0.86,0.836,0.888,0.854,0.872,0.916,0.894,0.918,0.894,0.864,0.89,0.888,0.852,0.866,0.86,0.88,0.91,0.904,0.892,0.886,0.914,0.89,0.902,0.908,0.874,0.834,0.9,0.952,0.86,0.86,0.894,0.884,0.838,0.888,0.854
1.106,1.16,1.08,1.12,1.136,1.136,1.152,1.126,1.154,1.168,1.154,1.156,1.168,1.168,1.174,1.194,1.156,1.18,1.18,1.21,1.18,1.216,1.22,1.182,1.214,1.212,1.19,1.222,1.206,1.206,1.204,1.22,1.202,1.212,1.192,1.218,1.222,1.212,1.226,1.224,1.212,1.202,1.222,1.218,1.21,1.228,1.238,1.216,1.226,1.242,1.244,1.214,1.216,1.222,1.212,1.228,1.228,1.234,1.212,1.232,1.23,1.218,1.22,1.222,1.228,1.226,1.242,1.22,1.222,1.232,1.194,1.242,1.234,1.22,1.234,1.224,1.252,1.244,1.248,1.244,1.238,1.232,1.236,1.224,1.224,1.222,1.226,1.224,1.24,1.26,1.22,1.236,1.232,1.224,1.238,1.23,1.222,1.254,1.238,1.234

val_acc1
1.17,1.22,1.14,1.2,1.07,1.04,0.99,0.92,0.93,0.93,0.92,0.93,0.92,1.06,0.97,0.91,0.95,0.89,1.01,0.93,0.88,0.91,0.91,0.89,0.88,0.85,0.85,0.87,0.86,0.95,0.86,0.86,0.92,0.88,0.94,0.88,0.89,0.91,0.85,0.85,0.85,0.85,0.85,0.88,0.88,0.86,0.86,0.87,0.87,0.86,0.93,0.84,0.91,0.85,0.86,0.87,0.86,0.85,0.88,0.9,0.86,0.86,0.9,0.87,0.85,0.86,0.85,0.92,0.82,0.83,0.86,0.9,0.86,0.85,0.87,0.86,0.87,0.87,0.87,0.85,0.85,0.87,0.86,0.86,0.85,0.87,0.86,0.87,0.85,0.86,0.85,0.91,0.85,0.84,0.87,0.9,0.8,0.86,0.9,0.87
1.16,1.13,1.14,1.17,1.14,1.15,1.1,1.18,1.17,1.16,1.13,1.18,1.19,1.19,1.17,1.19,1.18,1.19,1.2,1.17,1.18,1.22,1.21,1.21,1.16,1.19,1.17,1.17,1.16,1.22,1.24,1.19,1.2,1.24,1.19,1.21,1.17,1.21,1.23,1.21,1.19,1.22,1.23,1.21,1.2,1.23,1.23,1.23,1.19,1.22,1.22,1.22,1.23,1.22,1.23,1.24,1.23,1.24,1.24,1.23,1.24,1.22,1.23,1.24,1.24,1.22,1.22,1.23,1.24,1.25,1.25,1.24,1.25,1.25,1.25,1.23,1.24,1.25,1.23,1.26,1.28,1.26,1.25,1.24,1.24,1.24,1.24,1.24,1.24,1.25,1.26,1.26,1.26,1.24,1.25,1.28,1.26,1.24,1.25,1.26

val_loss
4.916,4.788,4.757,4.783,4.732,4.73,4.728,4.745,4.741,4.713,4.731,4.715,4.73,4.694,4.718,4.737,4.718,4.713,4.696,4.723,4.706,4.733,4.742,4.73,4.707,4.716,4.721,4.707,4.7,4.714,4.703,4.706,4.712,4.695,4.711,4.7,4.696,4.722,4.71,4.708,4.705,4.712,4.714,4.698,4.7,4.695,4.683,4.691,4.699,4.707,4.735,4.698,4.682,4.692,4.687,4.695,4.708,4.693,4.683,4.705,4.697,4.713,4.696,4.711,4.692,4.703,4.695,4.712,4.684,4.696,4.706,4.706,4.703,4.692,4.715,4.703,4.684,4.723,4.715,4.703,4.697,4.684,4.7,4.681,4.693,4.691,4.706,4.689,4.699,4.705,4.683,4.689,4.691,4.69,4.702,4.681,4.677,4.702,4.713,4.697
4.803,4.805,4.756,4.737,4.726,4.738,4.718,4.708,4.709,4.715,4.702,4.684,4.702,4.701,4.685,4.717,4.687,4.693,4.682,4.709,4.684,4.673,4.681,4.691,4.696,4.674,4.691,4.682,4.683,4.668,4.665,4.679,4.674,4.678,4.677,4.676,4.689,4.67,4.675,4.68,4.68,4.669,4.665,4.673,4.686,4.677,4.666,4.677,4.689,4.671,4.678,4.668,4.665,4.684,4.673,4.672,4.676,4.671,4.661,4.674,4.67,4.674,4.676,4.67,4.676,4.672,4.665,4.673,4.68,4.667,4.668,4.673,4.668,4.663,4.66,4.689,4.667,4.664,4.676,4.665,4.652,4.679,4.659,4.668,4.671,4.673,4.67,4.66,4.662,4.669,4.676,4.683,4.668,4.659,4.664,4.648,4.681,4.668,4.674,4.661

learning_rate
0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05
0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05

